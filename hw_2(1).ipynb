{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем PEP-8 проверку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pycodestyle flake8 pycodestyle_magic\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем нужные модули:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pymystem3\n",
    "! pip install pymorphy2\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "from pprint import pprint\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замеряем время работы, анализируем текст, сохраняем в формате json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('silly_frenchman.txt', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    ana = m.analyze(text) #анализируем текст с помощью mystem\n",
    "with open('mystem.json', 'w', encoding='utf-8') as file: \n",
    "    json.dump(ana, file, ensure_ascii = False) #сохраняем в json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замеряем время работы, токенизируем, парсим текст, достаем лемму и часть речи из первого разбора для каждого слова, сохраняем в формате json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ana_mas = []\n",
    "tok = word_tokenize(text) #токенизируем текст с помощью nltk\n",
    "tok_filtered = [w.lower() for w in tok if w.isalpha()] #приводим слова к нижнему регистру\n",
    "for tok in tok_filtered: \n",
    "    ana = morph.parse(tok) #парсим с помощью pymorphy\n",
    "    ana_mas.append(ana) \n",
    "lem_pos_mas = []\n",
    "for i in ana_mas:\n",
    "    first = i[0] #берем первый разбор слова\n",
    "    lem_pos = [first.normal_form, first.tag.POS] #берем из разбора лемму, часть речи\n",
    "    lem_pos_mas.append(lem_pos)\n",
    "with open('pymorphy.json', 'w', encoding='utf-8') as file: \n",
    "    json.dump(lem_pos_mas, file, ensure_ascii = False) #cохраняем в json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# №4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем файл из предыдущего пункта в формат питона, создадим массив, где будут лежать части речи, счетчик с частями речи и их кол-вом, посчитаем число слов и долю каждой части речи от общего числа слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lem_pos_mas = []\n",
    "for line in open('pymorphy.json', encoding='utf-8'): #преобразуем в питоновский формат\n",
    "    lem_pos_mas.extend(json.loads(line))\n",
    "pos_mas = [] #тут части речи\n",
    "for i in lem_pos_mas:\n",
    "    pos = i[1] \n",
    "    pos_mas.append(pos) #добавляем части речи в массив\n",
    "words_num = len(lem_pos_mas) #считаем число слов\n",
    "pos_counter = Counter(pos_mas) #считаем число частей речи\n",
    "pos_count_dict = dict(pos_counter) #преобразуем в словарь (\"часть речи\" - \"кол-во\")\n",
    "for key in pos_count_dict.keys():\n",
    "    #fraction = pos_count_dict[key]/words_num\n",
    "    pos_count_dict[key] = pos_count_dict[key]/words_num #считаем, какую долю слов составляет каждая часть речи\n",
    "print(pos_count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Топ-20 глаголов и наречий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем глаголы в счетчик, выводим 20 самых популярных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_count = collections.Counter() #подключим счетчик глаголов\n",
    "for i in lem_pos_mas:\n",
    "    if i[1] == 'VERB':\n",
    "        verb_count[i[0]] += 1 #если пометка глагола, добавляем в счетчик на первое место глагол\n",
    "        \n",
    "top_verb = verb_count.most_common(20)   #ищем 20 самых популярных глаголов\n",
    "for i in top_verb:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично для наречий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_count = collections.Counter()\n",
    "for i in lem_pos_mas:\n",
    "    if i[1] == 'ADVB':\n",
    "        adv_count[i[0]] += 1\n",
    "        \n",
    "top_adv = adv_count.most_common(20)      \n",
    "for i in top_adv:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# №5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставляем только леммы из массива с леммами и частью речи и записываем их в отдельный массив, ищем би- и триграммы, считаем их кол-во, оставляем 25 самых популярных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemms = []\n",
    "for i in lem_pos_mas:\n",
    "    lemms.append(i[0]) #вытаскиваем только леммы, добавляем их в массив\n",
    "bi = nltk.bigrams(lemms) #генерируем биграммы\n",
    "tri = nltk.trigrams(lemms) #генерируем триграммы\n",
    "bi_count = collections.Counter(bi).most_common(25) #вводим счетчик биграмм, оставляем 25 самых частотных\n",
    "tri_count = collections.Counter(tri).most_common(25) #вводим счетчик триграмм, оставляем 25 самых частотных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим би- и триграммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in bi_count:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tri_count:\n",
    "    print(i[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
