{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем PEP-8 проверку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycodestyle in c:\\programdata\\anaconda5\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: flake8 in c:\\programdata\\anaconda5\\lib\\site-packages (3.7.8)\n",
      "Requirement already satisfied: pycodestyle_magic in c:\\programdata\\anaconda5\\lib\\site-packages (0.4)\n",
      "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in c:\\programdata\\anaconda5\\lib\\site-packages (from flake8) (2.1.1)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in c:\\programdata\\anaconda5\\lib\\site-packages (from flake8) (0.3)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in c:\\programdata\\anaconda5\\lib\\site-packages (from flake8) (0.6.1)\n",
      "The pycodestyle_magic extension is already loaded. To reload it, use:\n",
      "  %reload_ext pycodestyle_magic\n"
     ]
    }
   ],
   "source": [
    "! pip install pycodestyle flake8 pycodestyle_magic\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Импортируем нужные модули:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in c:\\programdata\\anaconda5\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda5\\lib\\site-packages (from pymystem3) (2.21.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda5\\lib\\site-packages (from requests->pymystem3) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda5\\lib\\site-packages (from requests->pymystem3) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda5\\lib\\site-packages (from requests->pymystem3) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\programdata\\anaconda5\\lib\\site-packages (from requests->pymystem3) (1.24.1)\n",
      "Requirement already satisfied: pymorphy2 in c:\\programdata\\anaconda5\\lib\\site-packages (0.8)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\programdata\\anaconda5\\lib\\site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7 in c:\\programdata\\anaconda5\\lib\\site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in c:\\programdata\\anaconda5\\lib\\site-packages (from pymorphy2) (2.4.393442.3710985)\n"
     ]
    }
   ],
   "source": [
    "! pip install pymystem3\n",
    "! pip install pymorphy2\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "from pprint import pprint\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замеряем время работы, анализируем текст, сохраняем в формате json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('silly_frenchman.txt', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    ana = m.analyze(text) #анализируем текст с помощью mystem\n",
    "with open('mystem.json', 'w', encoding='utf-8') as file: \n",
    "    json.dump(ana, file, ensure_ascii = False) #сохраняем в json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замеряем время работы, токенизируем, парсим текст, достаем лемму и часть речи из первого разбора для каждого слова, сохраняем в формате json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "ana_mas = []\n",
    "tok = word_tokenize(text) #токенизируем текст с помощью nltk\n",
    "tok_filtered = [w.lower() for w in tok if w.isalpha()] #приводим слова к нижнему регистру\n",
    "for tok in tok_filtered: \n",
    "    ana = morph.parse(tok) #парсим с помощью pymorphy\n",
    "    ana_mas.append(ana) \n",
    "lem_pos_mas = []\n",
    "for i in ana_mas:\n",
    "    first = i[0] #берем первый разбор слова\n",
    "    lem_pos = [first.normal_form, first.tag.POS] #берем из разбора лемму, часть речи\n",
    "    lem_pos_mas.append(lem_pos)\n",
    "with open('pymorphy.json', 'w', encoding='utf-8') as file: \n",
    "    json.dump(lem_pos_mas, file, ensure_ascii = False) #cохраняем в json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# №4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем файл из предыдущего пункта в формат питона, создадим массив, где будут лежать части речи, счетчик с частями речи и их кол-вом, посчитаем число слов и долю каждой части речи от общего числа слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOUN': 0.24434389140271492, 'PREP': 0.08257918552036199, 'VERB': 0.15610859728506787, 'ADJF': 0.08257918552036199, 'INFN': 0.03619909502262444, 'NPRO': 0.08031674208144797, 'CONJ': 0.11877828054298642, 'PRED': 0.006787330316742082, 'ADVB': 0.05090497737556561, 'PRCL': 0.07805429864253394, 'PRTF': 0.0022624434389140274, 'INTJ': 0.007918552036199095, 'GRND': 0.019230769230769232, 'NUMR': 0.012443438914027148, 'ADJS': 0.014705882352941176, 'COMP': 0.0022624434389140274, None: 0.004524886877828055}\n"
     ]
    }
   ],
   "source": [
    "lem_pos_mas = []\n",
    "for line in open('pymorphy.json', encoding='utf-8'): #преобразуем в питоновский формат\n",
    "    lem_pos_mas.extend(json.loads(line))\n",
    "pos_mas = [] #тут части речи\n",
    "for i in lem_pos_mas:\n",
    "    pos = i[1] \n",
    "    pos_mas.append(pos) #добавляем части речи в массив\n",
    "words_num = len(lem_pos_mas) #считаем число слов\n",
    "pos_counter = Counter(pos_mas) #считаем число частей речи\n",
    "pos_count_dict = dict(pos_counter) #преобразуем в словарь (\"часть речи\" - \"кол-во\")\n",
    "for key in pos_count_dict.keys():\n",
    "    #fraction = pos_count_dict[key]/words_num\n",
    "    pos_count_dict[key] = pos_count_dict[key]/words_num #считаем, какую долю слов составляет каждая часть речи\n",
    "print(pos_count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Топ-20 глаголов и наречий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем глаголы в счетчик, выводим 20 самых популярных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мочь 11\n",
      "есть 9\n",
      "быть 6\n",
      "дать 5\n",
      "подать 4\n",
      "подумать 4\n",
      "съесть 4\n",
      "приказать 3\n",
      "подавать 2\n",
      "ужаснуться 2\n",
      "хотеть 2\n",
      "послушать 2\n",
      "удивиться 2\n",
      "сидеть 2\n",
      "стать 2\n",
      "делать 2\n",
      "знать 2\n",
      "думать 2\n",
      "иметь 2\n",
      "поглядеть 2\n"
     ]
    }
   ],
   "source": [
    "verb_count = collections.Counter() #подключим счетчик глаголов\n",
    "for i in lem_pos_mas:\n",
    "    if i[1] == 'VERB':\n",
    "        verb_count[i[0]] += 1 #если пометка глагола, добавляем в счетчик на первое место глагол\n",
    "        \n",
    "top_verb = verb_count.most_common(20)   #ищем 20 самых популярных глаголов\n",
    "for i in top_verb:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично для наречий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "много 9\n",
      "здесь 3\n",
      "странно 2\n",
      "уже 2\n",
      "уж 2\n",
      "зачем 2\n",
      "ужасно 2\n",
      "слишком 1\n",
      "сытно 1\n",
      "пока 1\n",
      "сразу 1\n",
      "едва 1\n",
      "неестественно 1\n",
      "сначала 1\n",
      "безнаказанно 1\n",
      "шёпотом 1\n",
      "этак 1\n",
      "сейчас 1\n",
      "тут 1\n",
      "нарочно 1\n"
     ]
    }
   ],
   "source": [
    "adv_count = collections.Counter()\n",
    "for i in lem_pos_mas:\n",
    "    if i[1] == 'ADVB':\n",
    "        adv_count[i[0]] += 1\n",
    "        \n",
    "top_adv = adv_count.most_common(20)      \n",
    "for i in top_adv:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# №5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставляем только леммы из массива с леммами и частью речи и записываем их в отдельный массив, ищем би- и триграммы, считаем их кол-во, оставляем 25 самых популярных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemms = []\n",
    "for i in lem_pos_mas:\n",
    "    lemms.append(i[0]) #вытаскиваем только леммы, добавляем их в массив\n",
    "bi = nltk.bigrams(lemms) #генерируем биграммы\n",
    "tri = nltk.trigrams(lemms) #генерируем триграммы\n",
    "bi_count = collections.Counter(bi).most_common(25) #вводим счетчик биграмм, оставляем 25 самых частотных\n",
    "tri_count = collections.Counter(tri).most_common(25) #вводим счетчик триграмм, оставляем 25 самых частотных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим би- и триграммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('много', 'есть')\n",
      "('не', 'мочь')\n",
      "('так', 'много')\n",
      "('благообразный', 'господин')\n",
      "('стол', 'и')\n",
      "('у', 'вы')\n",
      "('гора', 'блин')\n",
      "('и', 'неужели')\n",
      "('мочь', 'быть')\n",
      "('я', 'не')\n",
      "('с', 'пашот')\n",
      "('пожалуй', 'дать')\n",
      "('соседний', 'стол')\n",
      "('подумать', 'француз')\n",
      "('мочь', 'съесть')\n",
      "('он', 'к')\n",
      "('ещё', 'порция')\n",
      "('дядя', 'франсуа')\n",
      "('два', 'тарелка')\n",
      "('неужели', 'он')\n",
      "('дать', 'ещё')\n",
      "('бог', 'мой')\n",
      "('он', 'не')\n",
      "('хотеть', 'умереть')\n",
      "('вы', 'так')\n"
     ]
    }
   ],
   "source": [
    "for i in bi_count:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('соседний', 'стол', 'и')\n",
      "('и', 'неужели', 'он')\n",
      "('не', 'мочь', 'быть')\n",
      "('он', 'не', 'мочь')\n",
      "('так', 'много', 'есть')\n",
      "('вы', 'так', 'много')\n",
      "('сидеть', 'здесь', 'и')\n",
      "('за', 'стол', 'сидеть')\n",
      "('клоун', 'из', 'цирк')\n",
      "('из', 'цирк', 'брат')\n",
      "('цирк', 'брат', 'гинц')\n",
      "('брат', 'гинц', 'генри')\n",
      "('гинц', 'генри', 'пуркуа')\n",
      "('генри', 'пуркуа', 'зайти')\n",
      "('пуркуа', 'зайти', 'в')\n",
      "('зайти', 'в', 'московский')\n",
      "('в', 'московский', 'трактир')\n",
      "('московский', 'трактир', 'тестово')\n",
      "('трактир', 'тестово', 'позавтракать')\n",
      "('тестово', 'позавтракать', 'дать')\n",
      "('позавтракать', 'дать', 'я')\n",
      "('дать', 'я', 'консоме')\n",
      "('я', 'консоме', 'приказать')\n",
      "('консоме', 'приказать', 'он')\n",
      "('приказать', 'он', 'половый')\n"
     ]
    }
   ],
   "source": [
    "for i in tri_count:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
